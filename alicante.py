import requestsfrom bs4 import BeautifulSoupimport mysql.connectorfrom PyPDF2 import PdfReaderfrom io import BytesIOimport datetimefrom urllib3.exceptions import InsecureRequestWarningimport urllib3import timeurllib3.disable_warnings(InsecureRequestWarning)# Conectar a la base de datosconn = mysql.connector.connect(    host='localhost',    user='root',    password='',    database='justicio')cursor = conn.cursor()# URL de la página principalbase_url = 'https://www.alicante.es'url = f'{base_url}/es/normativa'# Obtener la página principal con verificación SSL deshabilitadadef get_soup(url):    print(f"\033[94mNavegando a la URL: {url}\033[0m")  # Imprimir en azul para seguimiento de navegación    response = requests.get(url, verify=False)    return BeautifulSoup(response.content, 'html.parser')def download_pdf(url):    attempts = 3    for attempt in range(attempts):        try:            response = requests.get(url, verify=False, stream=True)            response.raise_for_status()            if 'pdf' not in response.headers.get('Content-Type'):                raise Exception("URL no contiene un PDF")            return response.content        except (requests.exceptions.RequestException, Exception) as e:            print(f"Error al descargar el PDF, reintentando... ({attempt + 1}/{attempts}): {e}")            time.sleep(5)            continue    raise Exception(f"Failed to download PDF after {attempts} attempts")def extract_pdf_content(pdf_url):    try:        pdf_data = download_pdf(pdf_url)        pdf_content = ''        with BytesIO(pdf_data) as open_pdf_file:            reader = PdfReader(open_pdf_file)            for page_num in range(len(reader.pages)):                page = reader.pages[page_num]                text = page.extract_text()                if text:                    pdf_content += text                else:                    print(f"Advertencia: No se pudo extraer texto de la página {page_num + 1} del PDF")        # Codificar el contenido del PDF para la inserción        pdf_content_encoded = pdf_content.encode('utf-8', errors='replace').decode('utf-8')        return pdf_content_encoded    except Exception as e:        print(f"Error al procesar el PDF {pdf_url}: {e}")        return Nonedef process_pdf_links(pdf_links, grupo, subgrupo):    for pdf_link in pdf_links:        if '.pdf' in pdf_link['href']:            pdf_url = pdf_link['href']            if not pdf_url.startswith('http'):                pdf_url = base_url + pdf_url            pdf_title = pdf_link.text.strip()            print(f"\033[92mAccediendo a la URL del PDF: {pdf_url}\033[0m")  # Imprimir en verde para depuración            pdf_content_encoded = extract_pdf_content(pdf_url)            if pdf_content_encoded:                # Insertar en la base de datos                query = """                    INSERT INTO normativa (ciudad, date, titulo, grupo, subgrupo, url, content)                    VALUES (%s, %s, %s, %s, %s, %s, %s)                """                data = ('Alicante', current_date, pdf_title, grupo, subgrupo, pdf_url, pdf_content_encoded)                cursor.execute(query, data)                conn.commit()                print(f"\033[92mInsertado en la base de datos: {pdf_title}\033[0m")  # Imprimir en verde para depuracióndef process_page(url, grupo, subgrupo):    try:        soup = get_soup(url)        pdf_links = soup.find_all('a', href=True)        process_pdf_links(pdf_links, grupo, subgrupo)    except Exception as e:        print(f"Error al procesar la página {url}: {e}")def process_main_page(url):    soup = get_soup(url)    h3_elements = soup.find_all('h3')    for h3 in h3_elements:        spans = h3.find_all('span', class_='lineage-item')        grupo = spans[0].text.strip()        subgrupo = spans[1].text.strip() if len(spans) > 1 else None        links = h3.find_next_sibling('ul').find_all('a', href=True) if h3.find_next_sibling('ul') else []                for link in links:            href = link.get('href')            if href:                pdf_title = link.get('title', link.text).strip()                new_url = href                if new_url.startswith('/'):                    new_url = base_url + new_url                print(f"\033[92mAccediendo a la URL: {new_url}\033[0m")  # Imprimir en verde para depuración                print(f"\033[92mGrupo: {grupo}, Subgrupo: {subgrupo}\033[0m")  # Imprimir en verde para depuración                process_page(new_url, grupo, subgrupo)# Fecha actualcurrent_date = datetime.date.today()# Procesar la página principal y las paginacionesprocess_main_page(url)# Cerrar la conexión a la base de datoscursor.close()conn.close()