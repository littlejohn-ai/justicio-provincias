import requestsfrom bs4 import BeautifulSoupimport mysql.connectorfrom PyPDF2 import PdfReaderfrom io import BytesIOimport datetimefrom urllib3.exceptions import InsecureRequestWarningimport urllib3import timeurllib3.disable_warnings(InsecureRequestWarning)# Conectar a la base de datosconn = mysql.connector.connect(    host='localhost',    user='root',    password='',    database='justicio')cursor = conn.cursor()# URL de la página principalbase_url = 'https://institucional.cadiz.es'url = f'{base_url}/%C3%A1rea-municipal/ordenanzas-y-reglamentos'# Obtener la página principal con verificación SSL deshabilitadadef get_soup(url):    print(f"\033[94mNavegando a la URL: {url}\033[0m")  # Imprimir en azul para seguimiento de navegación    response = requests.get(url, verify=False)    return BeautifulSoup(response.content, 'html.parser')def download_pdf(url):    attempts = 3    for attempt in range(attempts):        try:            response = requests.get(url, verify=False, stream=True)            response.raise_for_status()            return response.content        except (requests.exceptions.ChunkedEncodingError, requests.exceptions.ConnectionError) as e:            print(f"Error al descargar el PDF, reintentando... ({attempt + 1}/{attempts})")            time.sleep(5)            continue    raise Exception(f"Failed to download PDF after {attempts} attempts")def process_page(url, grupo):    soup = get_soup(url)    pdf_link = soup.find('a', {'download': True, 'rel': 'nofollow'})    if pdf_link:        pdf_url = pdf_link['href']        pdf_title = pdf_link.text.strip()        print(f"\033[92mAccediendo a la URL del PDF: {pdf_url}\033[0m")  # Imprimir en verde para depuración        # Descargar y leer el contenido del PDF        try:            pdf_data = download_pdf(pdf_url)            pdf_content = ''                        with BytesIO(pdf_data) as open_pdf_file:                reader = PdfReader(open_pdf_file)                for page_num in range(len(reader.pages)):                    page = reader.pages[page_num]                    pdf_content += page.extract_text()            # Insertar en la base de datos            query = """                INSERT INTO normativa (ciudad, date, titulo, grupo, subgrupo, url, content)                VALUES (%s, %s, %s, %s, %s, %s, %s)            """            data = ('Cádiz', current_date, pdf_title, grupo, None, pdf_url, pdf_content)            cursor.execute(query, data)            conn.commit()            print(f"\033[92mInsertado en la base de datos: {pdf_title}\033[0m")  # Imprimir en verde para depuración        except Exception as e:            print(f"Error al procesar el PDF {pdf_url}: {e}")def process_main_page(url):    soup = get_soup(url)    initial_links = soup.select('a[title][rel="tag"]')    for link in initial_links:        href = link.get('href')        if href:            new_url = href            if new_url.startswith('/'):                new_url = base_url + new_url            grupo = link.get('title')            print(f"\033[92mAccediendo a la URL: {new_url}\033[0m")  # Imprimir en verde para depuración            print(f"\033[92mGrupo: {grupo}\033[0m")  # Imprimir en verde para depuración            process_page(new_url, grupo)    next_page_link = soup.find('a', {'title': 'Ir a la página siguiente'})    if next_page_link:        next_page_url = next_page_link['href']        if next_page_url.startswith('/'):            next_page_url = base_url + next_page_url        process_main_page(next_page_url)# Fecha actualcurrent_date = datetime.date.today()# Procesar la página principal y las paginacionesprocess_main_page(url)# Cerrar la conexión a la base de datoscursor.close()conn.close()