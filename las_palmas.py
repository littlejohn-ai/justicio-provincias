# -*- coding: utf-8 -*-
"""las_palmas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19WyjtXGvSz_MfUTOGJYSpCmwl7xAw33d
"""

!apt-get update

# servidor y clinte MySQL
# el nombre de base de datos y usuario se pueden cambiar
# he utilizado los del ejemplo para menor confusion

!apt-get install -y mysql-server mysql-client
!service mysql start
!mysql -e "CREATE DATABASE testdb;"
!mysql -e "CREATE USER 'testuser'@'localhost' IDENTIFIED BY 'testpassword';"
!mysql -e "GRANT ALL PRIVILEGES ON testdb.* TO 'testuser'@'localhost';"
!mysql -e "FLUSH PRIVILEGES;"

# instalacion del conector y la libraria fitz para trabajar con PDF
!pip install mysql-connector-python
!pip install PyMuPDF

# importamos las librerias
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import fitz
from io import BytesIO
import mysql.connector

# conectamos a la base de datos
# OJO, al cambiar nombre de base de datos, usuario y contraseña habra cambiarlos aqui tb
mydb = mysql.connector.connect(
    host="localhost",
    user="testuser",
    password="testpassword",
    database="testdb"
)

# cursor para ejecutar comandos SQL
mycursor = mydb.cursor()

# conectamos a la base de datos
# OJO, al cambiar nombre de base de datos, usuario y contraseña habra cambiarlos aqui tb
mydb = mysql.connector.connect(
    host="localhost",
    user="testuser",
    password="testpassword",
    database="testdb"
)

# cursor para ejecutar comandos SQL
mycursor = mydb.cursor()

url = 'https://www.laspalmasgc.es/es/areas-tematicas/urbanismo-e-infraestructuras/ordenanzas-y-normativa/'
ciudad = 'Las Palmas'
date = datetime.today().strftime('%Y-%m-%d')
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36'
}

response = requests.get(url, headers=headers)
soup = BeautifulSoup(response.content, 'html.parser')

# buscamos todos los enlaces a los archivos PDF en la pagina
filtered_urls = []
for a_tag in soup.find_all('a', href=True):
    href = a_tag['href']
    if href.endswith('.pdf'): # filtramos solo enlaces que terminan en .pdf
        pdf_url = 'https://www.laspalmasgc.es' + href # construimos la url completa del PDF
        title = a_tag.get_text(strip=True) # obtenemos el titulo
        pdf_response = requests.get(pdf_url)
        if pdf_response.status_code == 200 and 'application/pdf' in pdf_response.headers.get('Content-Type', ''):
            pdf_data = BytesIO(pdf_response.content)
            try:
                doc = fitz.open("pdf", pdf_data) # abrimos el PDF
                content = ""
                for page in doc: # iteramos dentro del PDF sobre cada pagina
                    text = page.get_text()
                    lines = text.split('\n')
                    lines = lines[1:-1] # eliminamos la cabecera y el pie del PDF
                    content += ' '.join(lines)
                filtered_urls.append({ # guardamos loas datos
                    'ciudad': ciudad,
                    'date': date,
                    'titulo': title,
                    'grupo': '',
                    'subgrupo': '',
                    'url': pdf_url,
                    'content': content
                })
            except fitz.FitzError:
                print(f"Error al abrir el archivo {pdf_url} como PDF.")
        else:
            print(f"Error al descargar el archivo {pdf_url}: {pdf_response.status_code}")

df = pd.DataFrame(filtered_urls)

# insertampos registros en la base de datos
for index, row in df.iterrows():
    sql = "INSERT INTO normativa (ciudad, date, titulo, grupo, subgrupo, url, content) VALUES (%s, %s, %s, %s, %s, %s, %s)"
    val = (row['ciudad'], row['date'], row['titulo'], row['grupo'], row['subgrupo'], row['url'], row['content'])
    mycursor.execute(sql, val)
    print(row['titulo'], " insertado en tabla.")
mydb.commit()

print(f"{len(df)} registro(s) insertado(s).")

# para verificar que los datos se han insertado correctamente
# hacemos un select y obtenemos el contenido de la tabla en BD
query = "SELECT * FROM normativa"
mycursor.execute(query)
rows = mycursor.fetchall()
columns = [col[0] for col in mycursor.description]
df = pd.DataFrame(rows, columns=columns)
df

# exportamos la base de datos en un archivo .sql

!mysql -u root -e "GRANT PROCESS, LOCK TABLES, SELECT ON *.* TO 'testuser'@'localhost'; FLUSH PRIVILEGES;"
!mysqldump -u testuser -ptestpassword testdb > /content/testdb.sql

from google.colab import files
import tarfile

with open('/content/normativa.sql', 'w') as f:
    f.write("USE testdb;\n")
    f.write("DROP TABLE IF EXISTS normativa;\n")
    f.write("""
    CREATE TABLE normativa (
        id INT UNSIGNED NOT NULL AUTO_INCREMENT,
        ciudad VARCHAR(100) DEFAULT NULL,
        date DATE DEFAULT NULL,
        titulo VARCHAR(255) DEFAULT NULL,
        grupo VARCHAR(255) DEFAULT NULL,
        subgrupo VARCHAR(255) DEFAULT NULL,
        url VARCHAR(255) DEFAULT NULL,
        content LONGTEXT,
        PRIMARY KEY (id)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
    """)

    for index, row in df.iterrows():
        f.write("INSERT INTO normativa (ciudad, date, titulo, grupo, subgrupo, url, content) VALUES ")
        content = row['content'] if pd.notnull(row['content']) else ""
        content_clean = content.replace('"', "'")
        f.write(f"('{row['ciudad']}', '{row['date']}', \"{row['titulo']}\", '{row['grupo']}', '{row['subgrupo']}', '{row['url']}', \"{content_clean}\");\n")

with tarfile.open('/content/testdb.tar.gz', 'w:gz') as tar:
    tar.add('/content/normativa.sql', arcname='normativa.sql')

files.download('/content/testdb.tar.gz')

query = "SELECT * FROM normativa"
mycursor.execute(query)
rows = mycursor.fetchall()
columns = [col[0] for col in mycursor.description]

df = pd.DataFrame(rows, columns=columns)
df

with open('/content/normativa.sql', 'w') as f:
    # Escribir el encabezado de la base de datos y la tabla
    f.write("USE testdb;\n")
    f.write("DROP TABLE IF EXISTS normativa;\n")
    f.write("""
    CREATE TABLE normativa (
        id INT UNSIGNED NOT NULL AUTO_INCREMENT,
        ciudad VARCHAR(100) DEFAULT NULL,
        date DATE DEFAULT NULL,
        titulo VARCHAR(255) DEFAULT NULL,
        grupo VARCHAR(255) DEFAULT NULL,
        subgrupo VARCHAR(255) DEFAULT NULL,
        url VARCHAR(255) DEFAULT NULL,
        content LONGTEXT,
        PRIMARY KEY (id)
    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci;
    """)
    for index, row in df.iterrows():
        f.write("INSERT INTO normativa (ciudad, date, titulo, grupo, subgrupo, url, content) VALUES ")
        # Reemplazar NaN por cadenas vacías y luego comillas dobles por comillas simples en el contenido del PDF
        content = row['content'] if pd.notnull(row['content']) else ""
        content_clean = content.replace('"', "'")
        # Utilizar triple comillas para evitar problemas con comillas en el contenido
        f.write(f"('{row['ciudad']}', '{row['date']}', \"{row['titulo']}\", '{row['grupo']}', '{row['subgrupo']}', '{row['url']}', \"{content_clean}\");\n")

with open('/content/normativa.sql', 'r') as f:
    print(f.read())

import tarfile

with tarfile.open('/content/testdb.tar.gz', 'w:gz') as tar:
    tar.add('/content/normativa.sql', arcname='normativa.sql')

files.download('/content/testdb.tar.gz')