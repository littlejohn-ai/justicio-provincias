import requestsfrom bs4 import BeautifulSoupimport mysql.connectorfrom PyPDF2 import PdfReaderfrom io import BytesIOimport datetimefrom urllib3.exceptions import InsecureRequestWarningimport urllib3import timeurllib3.disable_warnings(InsecureRequestWarning)# Conectar a la base de datosconn = mysql.connector.connect(    host='localhost',    user='root',    password='',    database='justicio')cursor = conn.cursor()# URL de la página principalbase_url = 'https://www.toledo.es'url = f'{base_url}/el-ayuntamiento/normativa/'# Obtener la página principal con verificación SSL deshabilitadadef get_soup(url):    print(f"\033[94mNavegando a la URL: {url}\033[0m")  # Imprimir en azul para seguimiento de navegación    response = requests.get(url, verify=False)    return BeautifulSoup(response.content, 'html.parser')def download_pdf(url):    attempts = 3    for attempt in range(attempts):        try:            response = requests.get(url, verify=False, stream=True)            response.raise_for_status()            return response.content        except (requests.exceptions.ChunkedEncodingError, requests.exceptions.ConnectionError) as e:            print(f"Error al descargar el PDF, reintentando... ({attempt + 1}/{attempts})")            time.sleep(5)            continue    raise Exception(f"Failed to download PDF after {attempts} attempts")def process_page(url, pdf_title, grupo):    try:        pdf_data = download_pdf(url)        pdf_content = ''        with BytesIO(pdf_data) as open_pdf_file:            reader = PdfReader(open_pdf_file)            for page_num in range(len(reader.pages)):                page = reader.pages[page_num]                text = page.extract_text()                if text:                    pdf_content += text                else:                    print(f"Advertencia: No se pudo extraer texto de la página {page_num + 1} del PDF")        # Codificar el contenido del PDF para la inserción        pdf_content_encoded = pdf_content.encode('utf-8', errors='replace').decode('utf-8')        # Insertar en la base de datos        query = """            INSERT INTO normativa (ciudad, date, titulo, grupo, subgrupo, url, content)            VALUES (%s, %s, %s, %s, %s, %s, %s)        """        data = ('Toledo', current_date, pdf_title, grupo, None, url, pdf_content_encoded)        cursor.execute(query, data)        conn.commit()        print(f"\033[92mInsertado en la base de datos: {pdf_title}\033[0m")  # Imprimir en verde para depuración    except Exception as e:        print(f"Error al procesar el PDF {url}: {e}")def process_main_page(url):    soup = get_soup(url)    h3_elements = soup.find_all('h3')    for h3 in h3_elements:        grupo = h3.text.strip()        links = h3.find_next_sibling('ul').find_all('a', href=True) if h3.find_next_sibling('ul') else []                for link in links:            href = link.get('href')            if href and href.endswith('.pdf'):                pdf_title = link.get('title', link.text).strip()                new_url = href                if new_url.startswith('/'):                    new_url = base_url + new_url                print(f"\033[92mAccediendo a la URL: {new_url}\033[0m")  # Imprimir en verde para depuración                print(f"\033[92mGrupo: {grupo}\033[0m")  # Imprimir en verde para depuración                process_page(new_url, pdf_title, grupo)# Fecha actualcurrent_date = datetime.date.today()# Procesar la página principal y las paginacionesprocess_main_page(url)# Cerrar la conexión a la base de datoscursor.close()conn.close()